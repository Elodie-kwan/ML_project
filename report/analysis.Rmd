---
output:
  pdf_document: default
  html_document: default
---

```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

# Analysis part of the project 

## Import dataset
Let's import the cleaned dataset that we created. 

```{r}
Mountain_data_cleaned <- read.csv("../data/Mountain_data_cleaned.csv")

Mountain_data_cleaned$Country <- as.factor(Mountain_data_cleaned$Country)
Mountain_data_cleaned$Mountain_range <- as.factor(Mountain_data_cleaned$Mountain_range)
Mountain_data_cleaned$Locality <- as.factor(Mountain_data_cleaned$Locality)
Mountain_data_cleaned$Plot <- as.factor(Mountain_data_cleaned$Plot)
Mountain_data_cleaned$Subplot <- as.factor(Mountain_data_cleaned$Subplot)

Mountain_data_cleaned$Date <- as.Date(Mountain_data_cleaned$Date)
```

## Replace the NAs by the mean of the closest observations
```{r}
mean_for_fill <-colMeans( Mountain_data_cleaned %>%
  select(Plot,Glu_P)%>%
  filter(Plot %in% c(76, 77))%>%na.omit()%>% select(Glu_P))
  
Mountain_data_cleaned[is.na(Mountain_data_cleaned)] <- mean_for_fill
rm(mean_for_fill)
```

## Splitting the data into Traning set and Test set 

```{r}
set.seed(123) ## for replication purpose

## the index of the rows that will be in the training set
index.tr <- sample(1:nrow(Mountain_data_cleaned), replace=FALSE,
                   size=0.75*nrow(Mountain_data_cleaned))

Mountain_data.tr_notsubs <- Mountain_data_cleaned[index.tr,] ## the training set
Mountain_data.te <- Mountain_data_cleaned[-index.tr,] ## the test set
```

## Balancing the Training set 
As discussed in the EDA part, we should balance our data because we do not have the same amount of information on each mountains. We have more observations on **Sierra de Guadarrama** and half less on **Central Andes**. 

```{r}
no.mountain_1 <- min(table(Mountain_data.tr_notsubs$Mountain_range)) ## 79

## the "Central Andes" cases
data.tr.mountain_1 <- filter(Mountain_data.tr_notsubs, Mountain_range=="Central Andes")

## the "Central Pyrenees" cases
data.tr.mountain_2 <- filter(Mountain_data.tr_notsubs, Mountain_range=="Central Pyrenees")

## The "Sierra de Guadarrama" cases 
data.tr.mountain_3 <- filter(Mountain_data.tr_notsubs, Mountain_range=="Sierra de Guadarrama") 
## sub-sample 79 instances from the number of "Central Pyrenees" cases
index.mountain_2 <- sample(size=no.mountain_1, 
                           x=1:nrow(data.tr.mountain_2), 
                           replace=FALSE)

## sub-sample 79 instances from the number of "Sierra de Guadarrama" cases
index.mountain_3 <- sample(size=no.mountain_1, 
                           x=1:nrow(data.tr.mountain_3), 
                           replace=FALSE)
  
## Bind all the "Central Andes" and the sub-sampled "Central Pyrenees" 
## and the sub-sampled "Sierra de Guadarrama"
Mountain_data.tr <- data.frame(rbind(data.tr.mountain_1,
                                     data.tr.mountain_2[index.mountain_2,],
                                     data.tr.mountain_3[index.mountain_3,])) 

## The cases are now balanced
table(Mountain_data.tr$Mountain_range)
```


## Neural Network Model 
Simple hyperparameter tuning, this code takes time to run.

```{r cache = TRUE, message = FALSE, warning = FALSE, results = 'hide'}
set.seed(1)
fitControl <- trainControl(method = "cv", 
                           number = 10)

nnetGrid <-  expand.grid(size = seq(from = 1, to = 6, by = 1),
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))

nnetFit <- train(Mountain_range ~ ., 
                 data = Mountain_data.tr,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

```

```{r}
plot(nnetFit)
```

The best Neural Networks parameters would be to choose 4 hidden layers, with a decay of 0.3. 

The manually written Neural Network model 
```{r}
set.seed(345)
nn4 <- nnet(Mountain_range ~ ., data=Mountain_data.tr, size=4, decay = 0.3)

nn4_pred <- predict(nn4, type="class")
tab4 <- table(Obs=Mountain_data.tr$Mountain_range, Pred=nn4_pred) # confusion matrix
tab4
(acc4 <- sum(diag(tab4))/sum(tab4)) # accuracy

```

Here it says that it has almost perfect accuracy (96%).

```{r}
# Confusion Matrix
confusionMatrix(data=as.factor(nn4_pred), reference = Mountain_data.tr$Mountain_range)
```

With this confusion matrix command we have more information on the model. As said before, we see that the accuracy is very high (96%) and we also see that we have a balanced accuracy of 1 which is the maximum we can get and which mean that our model do not suffer from unbalanced data. 


## Random Forest 
```{r}
train_set_for_RF <- Mountain_data.tr %>%
  select(!c(Plot, Subplot, Date, Day, Month, Year, Locality, Country))
test_set_for_RF <- Mountain_data.te%>%
  select(!c(Plot, Subplot, Date, Day, Month, Year, Locality, Country))

rf <- randomForest( Mountain_range ~ .,data=train_set_for_RF, importance = TRUE)
predtr <- predict(rf, newdata = train_set_for_RF[-1])
predte <-  predict(rf, newdata=test_set_for_RF[-1])
cmtr <-   table(train_set_for_RF[,1], predtr)      
cmte <-  table(test_set_for_RF[,1], predte)

varImpPlot(rf, cex = 0.65)
importance(rf)
Acc_tr <- sum(diag(cmtr))/sum(cmtr)
Acc_te <- sum(diag(cmte))/sum(cmte)

```

From the variable importance we see that the variable **pH_P**, **pH_T** and **pH_B** are very important. We can then say that the *pH* in general is important. 

```{r,  results='asis'}
cmtr
Acc_tr

cmte
Acc_te
```


## Naive Bayes 

Conditional density Plots with 'usekernel=TRUE' 
```{r}
mountain.nb <- naive_bayes(Mountain_range ~ .,
                       data = Mountain_data.tr%>%
  select(!c(Plot, Subplot, Date, Day, Month, Year, Locality, Country)), usekernel=TRUE, laplace=1)
#par(mfrow=c(2,2))
plot(mountain.nb, arg.num = list(col = 1:3,
                             legend.position = "topright",
                             legend.cex = 0.8),
     prob="conditional")
par(mfrow=c(1,1))
mountain.nb
```

From these density graphs we agree on what has been seen before, that the *pH* is important. *pH_T* is medium in Central Andes, lower in Sierra de Guadarrama and higher in Central Pyrenees. We can see that they have very different density for the three mountains so it can be a good feature to classify the mountains. 


Conditional density plots with 'uskernel=FALSE'. It means that Gaussian distribution is applied to 'numeric' vairable. 
```{r}
mountain.nb <- naive_bayes(Mountain_range ~ .,
                       data = Mountain_data.tr%>%
  select(!c(Plot, Subplot, Date, Day, Month, Year, Locality, Country)), usekernel=FALSE, laplace=1) 
#par(mfrow=c(2,2))
plot(mountain.nb, arg.num = list(col = 1:3,
                             legend.position = "topright",
                             legend.cex = 0.8), prob="conditional")
par(mfrow=c(1,1))
mountain.nb
```


Predictions 
```{r}
head(predict(mountain.nb, newdata = Mountain_data.tr[-c(1:9)]),20)
     
head(predict(mountain.nb, newdata = Mountain_data.tr[-c(1:9)], type="prob"),10)
```
Confusion matrix train data set 
```{r}
p1 <- predict(mountain.nb, Mountain_data.tr[-c(1:9)])
(tab1 <- table(p1, Mountain_data.tr$Mountain_range))
sum(diag(tab1)) / sum(tab1)
```
Confusion Matrix test data set 
```{r}
p2 <- predict(mountain.nb, Mountain_data.te[-c(1:9)])
(tab2 <- table(p2, Mountain_data.te$Mountain_range))
sum(diag(tab2)) / sum(tab2)
```
We can see from the confusion matrix that the accuracy is very high for the train data set, as well as for the test data set (99%). 

## K-NN Model

We use a 2-NN to predict the test set using the training set

```{R}
library(caret)
set.seed(123)
KNN <- knn3(data=Mountain_data.tr,Mountain_data.tr$Mountain_range ~ ., k=2)
MR.te.pred <- predict(KNN, newdata = Mountain_data.tr,type ="class") 

TAB <- table(Obs=Mountain_data.tr$Mountain_range, Pred=MR.te.pred) # confusion matrix 
TAB
(ACC <- sum(diag(TAB))/sum(TAB)) # accuracy 
```


### Unsupervised learning method, PCA


```{R}
library(plotly)

total <- cbind(Mountain_data_cleaned$Mountain_range, Mountain_data.num)

total <- total%>% na.omit()
pc <- princomp(total[,2:26], cor=TRUE, scores=TRUE)

summary(pc)
```
The first 2 components cover 63% of the total variance, with 3 dimensions 74%

```{R}
plot(pc, type="lines")
```
3 dimensions looks the best, the total within variance slightly decrease from there. 

```{R}

X <- Mountain_data.num%>% na.omit()

prin_comp <- prcomp(X, rank. = 3)

components <- prin_comp[["x"]]
components <- data.frame(components)
components$PC2 <- -components$PC2
components$PC3 <- -components$PC3
components = cbind(components, total$`Mountain_data_cleaned$Mountain_range`)

tit = 'Total Explained Variance = 74,03%'

fig <- plot_ly(components, x = ~PC1, y = ~PC2, z = ~PC3, color = ~total$`Mountain_data_cleaned$Mountain_range`, colors = c('#636EFA','#EF553B','#00CC96') ) %>%
  add_markers(size = 12)


fig <- fig %>%
  layout(
    title = tit,
    scene = list(bgcolor = "#e5ecf6")
)

fig
```

------------------------------------------

### CLuster analysis 

## Bootstrap 

## repliquer les mod√®les 

------------------------------------------

## Unique

First we remove every replicated data, a Unique dataset

```{r}
Mountain_df <- Mountain_data_cleaned[-c(4:9)]
Mountain_df <- unique(Mountain_df)
```

## Count

```{r}
ggplot(Mountain_df) +
  geom_bar(aes(x = Mountain_range))
```
No balancing of the data..

## Split in a new training set and test set

```{r}
set.seed(123) ## for replication purpose

# the index of the rows that will be in the training set
index.tr <- sample(1:nrow(Mountain_df), replace=FALSE,
                   size=0.75*nrow(Mountain_df))

Mountain_df.tr_notsubs <- Mountain_df[index.tr,] ## the training set
Mountain_df.te <- Mountain_df[-index.tr,] ## the test set
```

## Bootstrap
After we replicate with 100 replicates

```{r}
set.seed(897)
index.boot <- createResample(y=Mountain_df.tr_notsubs$Mountain_range, times=100)
head(index.boot[[1]])
tail(index.boot[[1]])
```

```{r}
df.boot.tr <- Mountain_df.tr_notsubs[index.boot[[1]],]
dim(df.boot.tr)

df.boot.val <- Mountain_df.tr_notsubs[-index.boot[[1]],]
dim(df.boot.val)
```

```{r}
# The count of the bootstraped df
ggplot(df.boot.tr) +
+     geom_bar(aes(x = Mountain_range))
```




