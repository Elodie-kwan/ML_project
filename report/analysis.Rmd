---
output:
  pdf_document: default
  html_document: default
---

```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

# Analysis part of the project 

## Import dataset
Let's import the cleaned dataset that we created. 

```{r}
Mountain_data_cleaned <- read.csv("../data/Mountain_data_cleaned.csv")

Mountain_data_cleaned$Country <- as.factor(Mountain_data_cleaned$Country)
Mountain_data_cleaned$Mountain_range <- as.factor(Mountain_data_cleaned$Mountain_range)
Mountain_data_cleaned$Locality <- as.factor(Mountain_data_cleaned$Locality)
Mountain_data_cleaned$Plot <- as.factor(Mountain_data_cleaned$Plot)
Mountain_data_cleaned$Subplot <- as.factor(Mountain_data_cleaned$Subplot)

Mountain_data_cleaned$Date <- as.Date(Mountain_data_cleaned$Date)
```

## Replace the NAs by the mean of the closest observations
```{r}
mean_for_fill <-colMeans( Mountain_data_cleaned %>%
  select(Plot,Glu_P)%>%
  filter(Plot %in% c(76, 77))%>%na.omit()%>% select(Glu_P))
  
Mountain_data_cleaned[is.na(Mountain_data_cleaned)] <- mean_for_fill
rm(mean_for_fill)
```

## Splitting the data into Traning set and Test set 

```{r}
set.seed(123) ## for replication purpose

## the index of the rows that will be in the training set
index.tr <- sample(1:nrow(Mountain_data_cleaned), replace=FALSE,
                   size=0.75*nrow(Mountain_data_cleaned))

Mountain_data.tr_notsubs <- Mountain_data_cleaned[index.tr,] ## the training set
Mountain_data.te <- Mountain_data_cleaned[-index.tr,] ## the test set
```

## Balancing the Training set 
As discussed in the EDA part, we should balance our data because we do not have the same amount of information on each mountains. We have more observations on **Sierra de Guadarrama** and half less on **Central Andes**. 

```{r}
no.mountain_1 <- min(table(Mountain_data.tr_notsubs$Mountain_range)) ## 79

## the "Central Andes" cases
data.tr.mountain_1 <- filter(Mountain_data.tr_notsubs, Mountain_range=="Central Andes")

## the "Central Pyrenees" cases
data.tr.mountain_2 <- filter(Mountain_data.tr_notsubs, Mountain_range=="Central Pyrenees")

## The "Sierra de Guadarrama" cases 
data.tr.mountain_3 <- filter(Mountain_data.tr_notsubs, Mountain_range=="Sierra de Guadarrama") 
## sub-sample 79 instances from the number of "Central Pyrenees" cases
index.mountain_2 <- sample(size=no.mountain_1, 
                           x=1:nrow(data.tr.mountain_2), 
                           replace=FALSE)

## sub-sample 79 instances from the number of "Sierra de Guadarrama" cases
index.mountain_3 <- sample(size=no.mountain_1, 
                           x=1:nrow(data.tr.mountain_3), 
                           replace=FALSE)
  
## Bind all the "Central Andes" and the sub-sampled "Central Pyrenees" 
## and the sub-sampled "Sierra de Guadarrama"
Mountain_data.tr <- data.frame(rbind(data.tr.mountain_1,
                                     data.tr.mountain_2[index.mountain_2,],
                                     data.tr.mountain_3[index.mountain_3,])) 

## The cases are now balanced
table(Mountain_data.tr$Mountain_range)
```


## Neural Network Model 
Simple hyperparameter tuning, this code takes time to run.

```{r cache = TRUE, message = FALSE, warning = FALSE, results = 'hide'}
set.seed(1)
fitControl <- trainControl(method = "cv", 
                           number = 10)

nnetGrid <-  expand.grid(size = seq(from = 1, to = 6, by = 1),
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))

nnetFit <- train(Mountain_range ~ ., 
                 data = Mountain_data.tr,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

```

```{r}
plot(nnetFit)
```

The best Neural Networks parameters would be to choose 4 hidden layers, with a decay of 0.3. 

The manually written Neural Network model 
```{r}
set.seed(345)
nn4 <- nnet(Mountain_range ~ ., data=Mountain_data.tr, size=4, decay = 0.3)

nn4_pred <- predict(nn4, type="class")
tab4 <- table(Obs=Mountain_data.tr$Mountain_range, Pred=nn4_pred) # confusion matrix
tab4
(acc4 <- sum(diag(tab4))/sum(tab4)) # accuracy

```

Here it says that it has almost perfect accuracy (96%).

```{r}
# Confusion Matrix
confusionMatrix(data=as.factor(nn4_pred), reference = Mountain_data.tr$Mountain_range)
```

With this confusion matrix command we have more information on the model. As said before, we see that the accuracy is very high (96%) and we also see that we have a balanced accuracy of 1 which is the maximum we can get and which mean that our model do not suffer from unbalanced data. 


## Random Forest 
```{r}
train_set_for_RF <- Mountain_data.tr %>%
  select(!c(Plot, Subplot, Date, Day, Month, Year, Locality, Country))
test_set_for_RF <- Mountain_data.te%>%
  select(!c(Plot, Subplot, Date, Day, Month, Year, Locality, Country))

rf <- randomForest( Mountain_range ~ .,data=train_set_for_RF, importance = TRUE)
predtr <- predict(rf, newdata = train_set_for_RF[-1])
predte <-  predict(rf, newdata=test_set_for_RF[-1])
cmtr <-   table(train_set_for_RF[,1], predtr)      
cmte <-  table(test_set_for_RF[,1], predte)

varImpPlot(rf, cex = 0.65)
importance(rf)
Acc_tr <- sum(diag(cmtr))/sum(cmtr)
Acc_te <- sum(diag(cmte))/sum(cmte)

```

From the variable importance we see that the variable **pH_P**, **pH_T** and **pH_B** are very important. We can then say that the *pH* in general is important. 

```{r,  results='asis'}
cmtr
Acc_tr

cmte
Acc_te
```


## Naive Bayes 

The analysis of the "Naive Bayes" model comes back to the analysis of the density graphs of each variable by mountain range.
From these density graphs we agree on what has been seen before, that the *pH* is a very good feature to classify the mountains. Indeed, *pH_T* *ph_B* and *ph_P* is medium in Central Andes, lower in Sierra de Guadarrama and higher in Central Pyrenees. 

In addition to that, the observation of other variables such as phosphatase enzyme, (*Phos_P, Phos_B, Phos_T*), β-glucosidase enzyme (*Glu_B*, *Glu_P*, *Glu_T*) , soil organic carbon (*SOC_T*, *SOC_B*, *SOC_P*), soil total nitrogen (*NT_P*,*NT_B*,*NT_T*), electrical conductivity (*Cond_P*,*Cond_B*, *Cond_T*), and the *radiation* could allow us to get an idea of the mountain we are on.
Indeed, a radiation lower than 0,6 indicates rather the Central Pyrenees. The electrical conductivity allows us to distinguish the Central Andes from the Central Pyrenees, since it is higher for the latter. 
If we look at the soil total nitrogen, we see that it is lower for the Central Andes than for the two others. 
The soil organic carbon allows us to distinguish the Central Andes from the Sierra de Guadarrama, since it is higher for this last one.
The β-glucosidase enzyme is present at approximately the same level in Central Andes and Central Pyrenees but has a higher value in Sierra de Guadarrama. It is about the same for the phosphatase enzyme 

However, some variables do not allow us to determine the mountain at all. It is the case by observing the phosphorus (*PT_P, PT_B, PT_T*) or the potassium content (*K_P, K_B, K_T*).


## K-NN Model

We use a 2-NN to predict the test set using the training set

```{R}
library(caret)
set.seed(123)
KNN <- knn3(data=Mountain_data.tr,Mountain_data.tr$Mountain_range ~ ., k=2)
MR.te.pred <- predict(KNN, newdata = Mountain_data.tr,type ="class") 

TAB <- table(Obs=Mountain_data.tr$Mountain_range, Pred=MR.te.pred) # confusion matrix 
TAB
(ACC <- sum(diag(TAB))/sum(TAB)) # accuracy 
```


### Unsupervised learning method, PCA


```{R}
library(plotly)

total <- cbind(Mountain_data_cleaned$Mountain_range, Mountain_data.num)

total <- total%>% na.omit()
pc <- princomp(total[,2:26], cor=TRUE, scores=TRUE)

summary(pc)
```
The first 2 components cover 63% of the total variance, with 3 dimensions 74%

```{R}
plot(pc, type="lines")
```
3 dimensions looks the best, the total within variance slightly decrease from there. 

```{R}

X <- Mountain_data.num%>% na.omit()

prin_comp <- prcomp(X, rank. = 3)

components <- prin_comp[["x"]]
components <- data.frame(components)
components$PC2 <- -components$PC2
components$PC3 <- -components$PC3
components = cbind(components, total$`Mountain_data_cleaned$Mountain_range`)

tit = 'Total Explained Variance = 74,03%'

fig <- plot_ly(components, x = ~PC1, y = ~PC2, z = ~PC3, color = ~total$`Mountain_data_cleaned$Mountain_range`, colors = c('#636EFA','#EF553B','#00CC96') ) %>%
  add_markers(size = 12)


fig <- fig %>%
  layout(
    title = tit,
    scene = list(bgcolor = "#e5ecf6")
)

fig
```

------------------------------------------

### CLuster analysis 

## Bootstrap 

## repliquer les modèles 

------------------------------------------

## Unique

First we remove every replicated data, a Unique dataset

```{r}
Mountain_df <- Mountain_data_cleaned[-c(4:9)]
Mountain_df <- unique(Mountain_df)
```

## Count

```{r}
ggplot(Mountain_df) +
  geom_bar(aes(x = Mountain_range))
```
No balancing of the data..

## Split in a new training set and test set

```{r}
set.seed(123) ## for replication purpose

# the index of the rows that will be in the training set
index.tr <- sample(1:nrow(Mountain_df), replace=FALSE,
                   size=0.75*nrow(Mountain_df))

Mountain_df.tr_notsubs <- Mountain_df[index.tr,] ## the training set
Mountain_df.te <- Mountain_df[-index.tr,] ## the test set
```

## Bootstrap
After we replicate with 100 replicates

```{r}
set.seed(897)
index.boot <- createResample(y=Mountain_df.tr_notsubs$Mountain_range, times=100)
head(index.boot[[1]])
tail(index.boot[[1]])
```

```{r}
df.boot.tr <- Mountain_df.tr_notsubs[index.boot[[1]],]
dim(df.boot.tr)

df.boot.val <- Mountain_df.tr_notsubs[-index.boot[[1]],]
dim(df.boot.val)
```

```{r}
# The count of the bootstraped df
ggplot(df.boot.tr) +
+     geom_bar(aes(x = Mountain_range))
```


## Cluster Analysis


```{r}

row.names(Mountain_data_cleaned) <- paste("M", c(1:nrow(Mountain_data_cleaned)), sep="") # row names are used after
head(Mountain_data_cleaned)
summary(Mountain_data_cleaned)
Mountain_data_cleaned[,-c(1:9)] <- scale(Mountain_data_cleaned[,-c(1:9)])


#agglomerative hierarchical clustering (AGNES)with Manhattan distance

mountain.d <- dist(Mountain_data_cleaned[,-c(1:9)], method = "manhattan") # matrix of Manhattan distances 

mountain.melt <- melt(as.matrix(mountain.d)) # create a data frame of the distances in long format
head(mountain.melt)

ggplot(data = mountain.melt, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() 

#dendrogram using a complete linkage.
mountain.hc <- hclust(mountain.d, method = "complete")
plot(mountain.hc, hang=-1)
#cut the tree to 4 clusters
plot(mountain.hc, hang=-1)
rect.hclust(mountain.hc, k=4)
mountain.clust <- cutree(mountain.hc, k=5)
mountain.clust

#Interpretation of the clusters
mountain.comp <- data.frame(Mountain_data.num, Clust=factor(mountain.clust) ,Id=row.names(Mountain_data_cleaned))
mountain.df <- melt(mountain.comp, id=c("Id", "Clust"))
head(mountain.df)

ggplot(mountain.df, aes(y=value, group=Clust, fill=Clust)) +
  geom_boxplot()
  facet_wrap(~variable, ncol=4, nrow=3)

#number of clusters

fviz_nbclust(Mountain_data_cleaned[,-c(1:9)],
             hcut, hc_method="complete",
             hc_metric="manhattan",
             method = "silhouette", 
             k.max = 25, verbose = FALSE)
library(cluster)

mountain.pam <- pam(Mountain_data_cleaned[,-c(1:9)], k=5)
mountain.pam
plot(silhouette(mountain.pam))
```


