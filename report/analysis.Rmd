---
output:
  html_document: default
  pdf_document: default
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
source("../scripts/setup.R")
```

## Part 3 : Ananlysis 

### Import dataset
Let's import the cleaned dataset that we created. 

```{r}
Mountain_data_cleaned <- read.csv("../data/Mountain_data_cleaned.csv")
```

```{r , echo = FALSE, message = FALSE, warning = FALSE}
Mountain_data_cleaned$Country <- as.factor(Mountain_data_cleaned$Country)
Mountain_data_cleaned$Mountain_range <- as.factor(Mountain_data_cleaned$Mountain_range)
Mountain_data_cleaned$Locality <- as.factor(Mountain_data_cleaned$Locality)
Mountain_data_cleaned$Plot <- as.factor(Mountain_data_cleaned$Plot)
Mountain_data_cleaned$Subplot <- as.factor(Mountain_data_cleaned$Subplot)

Mountain_data_cleaned$Date <- as.Date(Mountain_data_cleaned$Date)
```

### Replace the NAs by the mean of the closest observations
Some of the models we use do not work with NAs. To deal with them, we decided to replace the 2 NAs we have by the mean of their closest observations. 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
mean_for_fill <-colMeans( Mountain_data_cleaned %>%
  select(Plot,Glu_P)%>%
  filter(Plot %in% c(76, 77))%>%na.omit()%>% select(Glu_P))
  
Mountain_data_cleaned[is.na(Mountain_data_cleaned)] <- mean_for_fill
rm(mean_for_fill)
```

### Splitting the data into Traning set and Test set 
The next step is to split our data set into a training set (**Mountain_data.tr_notsubs**) and a test set (**Mountain_data.te**). The training set has 75% of the observations and the test set has the 25% remainder.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(123) ## for replication purpose

## the index of the rows that will be in the training set
index.tr <- sample(1:nrow(Mountain_data_cleaned), replace=FALSE,
                   size=0.75*nrow(Mountain_data_cleaned))

Mountain_data.tr_notsubs <- Mountain_data_cleaned[index.tr,] ## the training set
Mountain_data.te <- Mountain_data_cleaned[-index.tr,] ## the test set
```

### Balancing the Training set 
As discussed in the EDA part, we should balance our data because we do not have the same amount of information on each mountains. We have more observations on **Sierra de Guadarrama** and half less on **Central Andes**. 

We balanced the data on the training set **Mountain_data.tr_notsubs**.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
no.mountain_1 <- min(table(Mountain_data.tr_notsubs$Mountain_range)) ## 79

## the "Central Andes" cases
data.tr.mountain_1 <- filter(Mountain_data.tr_notsubs, Mountain_range=="Central Andes")

## the "Central Pyrenees" cases
data.tr.mountain_2 <- filter(Mountain_data.tr_notsubs, Mountain_range=="Central Pyrenees")

## The "Sierra de Guadarrama" cases 
data.tr.mountain_3 <- filter(Mountain_data.tr_notsubs, Mountain_range=="Sierra de Guadarrama") 
## sub-sample 79 instances from the number of "Central Pyrenees" cases
index.mountain_2 <- sample(size=no.mountain_1, 
                           x=1:nrow(data.tr.mountain_2), 
                           replace=FALSE)

## sub-sample 79 instances from the number of "Sierra de Guadarrama" cases
index.mountain_3 <- sample(size=no.mountain_1, 
                           x=1:nrow(data.tr.mountain_3), 
                           replace=FALSE)
  
## Bind all the "Central Andes" and the sub-sampled "Central Pyrenees" 
## and the sub-sampled "Sierra de Guadarrama"
Mountain_data.tr <- data.frame(rbind(data.tr.mountain_1,
                                     data.tr.mountain_2[index.mountain_2,],
                                     data.tr.mountain_3[index.mountain_3,])) 

## The cases are now balanced
table(Mountain_data.tr$Mountain_range)
```

Now we see that all three mountains has the same amount of observations. We decided to sub-samples the observations because we think that duplicating them will create more biases as we do not have that many observations. 

Our new training set is **Mountain_data.tr**. 

### Neural Network Model 

We start by fitting a Neural Network model on our balaned training set (**Mountain_data.tr**). In order to choose the parameters of this neural network, we applied a simple hyperparameter tuning. 

```{r cache = TRUE, message = FALSE, warning = FALSE, results = 'hide'}
# this code takes time to run
set.seed(1)
fitControl <- trainControl(method = "cv", 
                           number = 10)

nnetGrid <-  expand.grid(size = seq(from = 1, to = 6, by = 1),
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))

nnetFit <- train(Mountain_range ~ ., 
                 data = Mountain_data.tr,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
plot(nnetFit)
```

The best Neural Networks parameters would be to choose 4 hidden layers, with a decay of 0.1. 

The manually written Neural Network model 
```{r, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(345)
nn4 <- nnet(Mountain_range ~ ., data=Mountain_data.tr, size=4, decay = 0.1)

nn4_pred <- predict(nn4, Mountain_data.te, type="class")
tab4 <- table(Obs=Mountain_data.te$Mountain_range, Pred=nn4_pred) # confusion matrix
tab4
(acc4 <- sum(diag(tab4))/sum(tab4)) # accuracy

```

Here it says that it has almost perfect accuracy (99%).

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Confusion Matrix
confusionMatrix(data=as.factor(nn4_pred), reference = Mountain_data.te$Mountain_range)
```

With this confusion matrix command we have more information on the model. As said before, we see that the accuracy is very high (99%) and we also see that we have a balanced accuracy of 1 which is the maximum we can get and which mean that our model do not suffer from unbalanced data. 


### Random Forest 
```{r, echo = FALSE, message = FALSE, warning = FALSE}
train_set_for_RF <- Mountain_data.tr %>%
  select(!c(Plot, Subplot, Date, Day, Month, Year, Locality, Country))
test_set_for_RF <- Mountain_data.te%>%
  select(!c(Plot, Subplot, Date, Day, Month, Year, Locality, Country))

rf <- randomForest( Mountain_range ~ .,data=train_set_for_RF, importance = TRUE)
predtr <- predict(rf, newdata = train_set_for_RF[-1])
predte <-  predict(rf, newdata=test_set_for_RF[-1])
cmtr <-   table(train_set_for_RF[,1], predtr)      
cmte <-  table(test_set_for_RF[,1], predte)

varImpPlot(rf, cex = 0.65)
importance(rf)
Acc_tr <- sum(diag(cmtr))/sum(cmtr)
Acc_te <- sum(diag(cmte))/sum(cmte)

```

From the variable importance we see that the variable **pH_P**, **pH_T** and **pH_B** are very important. We can then say that the *pH* in general is important. 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Confusion Matrix of the Random Forest model on training set 
cmtr
Acc_tr

# Confusion Matrix of the Random Forest model on training set 
cmte
Acc_te
```

The first table is the confusion matrix of the Random Forest model on the training set. Therefore it is normal to have an accuracy of 1 since the model already knows the observations. 

The second table is the confusion matrix of the Random Forest model on the test set. 
We see that the model has a accuracy of 99% when it comes to predict new observations. 

### Naive Bayes 

#### PH
```{r, echo = FALSE, message = FALSE, warning = FALSE}

d1 <- Mountain_data_cleaned %>% ggplot(aes(x =pH_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

d2 <- Mountain_data_cleaned %>% ggplot(aes(x =pH_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

d3 <- Mountain_data_cleaned %>% ggplot(aes(x =pH_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

d1 / d2 / d3
```

#### Phosphatase enzyme

```{r, echo = FALSE, message = FALSE, warning = FALSE}
x1 <- Mountain_data_cleaned %>% ggplot(aes(x =Phos_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

x2 <- Mountain_data_cleaned %>% ggplot(aes(x =Phos_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

x3 <- Mountain_data_cleaned %>% ggplot(aes(x =Phos_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

x1 / x2 / x3
```

#### β-glucosidase enzyme

```{r, echo = FALSE, message = FALSE, warning = FALSE}
y1 <- Mountain_data_cleaned %>% ggplot(aes(x =Glu_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

y2 <- Mountain_data_cleaned %>% ggplot(aes(x =Glu_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

y3 <- Mountain_data_cleaned %>% ggplot(aes(x =Glu_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

y1 / y2 / y3
```

#### soil organic carbon

```{r, echo = FALSE, message = FALSE, warning = FALSE}
z1 <- Mountain_data_cleaned %>% ggplot(aes(x =SOC_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

z2 <- Mountain_data_cleaned %>% ggplot(aes(x =SOC_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

z3 <- Mountain_data_cleaned %>% ggplot(aes(x =SOC_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

z1 / z2 / z3
```

#### Phosphatase enzyme

```{r, echo = FALSE, message = FALSE, warning = FALSE}
x1 <- Mountain_data_cleaned %>% ggplot(aes(x =Phos_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

x2 <- Mountain_data_cleaned %>% ggplot(aes(x =Phos_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

x3 <- Mountain_data_cleaned %>% ggplot(aes(x =Phos_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

x1 / x2 / x3
```

#### β-glucosidase enzyme

```{r, echo = FALSE, message = FALSE, warning = FALSE}
y1 <- Mountain_data_cleaned %>% ggplot(aes(x =Glu_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

y2 <- Mountain_data_cleaned %>% ggplot(aes(x =Glu_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

y3 <- Mountain_data_cleaned %>% ggplot(aes(x =Glu_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

y1 / y2 / y3
```

#### soil organic carbon

```{r, echo = FALSE, message = FALSE, warning = FALSE}
z1 <- Mountain_data_cleaned %>% ggplot(aes(x =SOC_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

z2 <- Mountain_data_cleaned %>% ggplot(aes(x =SOC_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

z3 <- Mountain_data_cleaned %>% ggplot(aes(x =SOC_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

z1 / z2 / z3
```

#### soil total nitrogen

```{r, echo = FALSE, message = FALSE, warning = FALSE}
a1 <- Mountain_data_cleaned %>% ggplot(aes(x =NT_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

a2 <- Mountain_data_cleaned %>% ggplot(aes(x =NT_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

a3 <- Mountain_data_cleaned %>% ggplot(aes(x =NT_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

a1 / a2 / a3
```

#### electrical conductivity

```{r, echo = FALSE, message = FALSE, warning = FALSE}

e1 <- Mountain_data_cleaned %>% ggplot(aes(x =Cond_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

e2 <- Mountain_data_cleaned %>% ggplot(aes(x =Cond_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

e3 <- Mountain_data_cleaned %>% ggplot(aes(x =Cond_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

e1 / e2 / e3
```

#### radiation

```{r, echo = FALSE, message = FALSE, warning = FALSE}
f1 <- Mountain_data_cleaned %>% ggplot(aes(x =Radiation, fill= `Mountain_range`))+geom_density(alpha = 0.3)
f1
```

The analysis of the "Naive Bayes" model comes back to the analysis of the density graphs of each variable by mountain range.
From these density graphs we agree on what has been seen before, that the *pH* is a very good feature to classify the mountains. Indeed, *pH_T* *ph_B* and *ph_P* is medium in Central Andes, lower in Sierra de Guadarrama and higher in Central Pyrenees. 

In addition to that, the observation of other variables such as phosphatase enzyme, (*Phos_P, Phos_B, Phos_T*), β-glucosidase enzyme (*Glu_B*, *Glu_P*, *Glu_T*) , soil organic carbon (*SOC_T*, *SOC_B*, *SOC_P*), soil total nitrogen (*NT_P*,*NT_B*,*NT_T*), electrical conductivity (*Cond_P*,*Cond_B*, *Cond_T*), and the *radiation* could allow us to get an idea of the mountain we are on.
Indeed, a radiation lower than 0,6 indicates rather the Central Pyrenees. The electrical conductivity allows us to distinguish the Central Andes from the Central Pyrenees, since it is higher for the latter. 
If we look at the soil total nitrogen, we see that it is lower for the Central Andes than for the two others. 
The soil organic carbon allows us to distinguish the Central Andes from the Sierra de Guadarrama, since it is higher for this last one.
The β-glucosidase enzyme is present at approximately the same level in Central Andes and Central Pyrenees but has a higher value in Sierra de Guadarrama. It is about the same for the phosphatase enzyme 

However, some variables do not allow us to determine the mountain at all. It is the case by observing the phosphorus (*PT_P, PT_B, PT_T*) or the potassium content (*K_P, K_B, K_T*).


### K-NN Model

We use a 2-NN to predict the test set using the training set

```{r, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(123)
KNN <- knn3(data=Mountain_data.tr,Mountain_data.tr$Mountain_range ~ ., k=2)
MR.te.pred <- predict(KNN, newdata = Mountain_data.te,type ="class") 

TAB <- table(Obs=Mountain_data.te$Mountain_range, Pred=MR.te.pred) # confusion matrix 
TAB
(ACC <- sum(diag(TAB))/sum(TAB)) # accuracy 
```

With this KNN supervised method, the prediction made on the test set gives us an accuracy of 1. It looks indeed perfect and attractive, but it is complytely biased by the fact that subplot can be identical to plot. Therefore, there is an overlay in the pbservations and the distance computed is 0. What we need to do is to make a unite dataset with no observations overlayed.

----------------------------------------------------------------------------------------------------------------------------------------------------------


### Analysis with unique dataset 

We now want to replicate the above models with a another training set and test set that has been through a boostraping method. To do so, we first want to delete all the repeated observations from the original cleaned dataset. 

#### Unique

We remove every replicated data with the "unique" function. We decided to do so to see if we will have better results. As we see above, the accuracy of our models are very high and this is normally unlikely. So we are testing if the high accuracy is due to the fact that we have duplicated observations in our dataset. (The scientists did sample different observations, but it comes out that the samples are exactly the same, they are maybe different at a very very small digits.)

```{r, echo = FALSE, message = FALSE, warning = FALSE}
Mountain_df <- Mountain_data_cleaned[-c(4:9)]
Mountain_df <- unique(Mountain_df)
```

#### Count

We recount the number of observations after having removed all the duplicated observations.

```{r}
ggplot(Mountain_df) +
  geom_bar(aes(x = Mountain_range))
```

Here we see the new Unique dataset and notice that it is largely unbalanced. The mountain that has the most information is now the one with the less observations, so information.

```{r}
table(Mountain_df$Mountain_range)
```

Sierre de Gaudarrame now only have 39 observations. It is unbalanced indeed, and insufficient to make accurate classification methods. 

#### Split in a new training set and test set

Now that we have a new dataset, we have to split it again into a training set (**Mountain_df.tr_notsubs**) and a test set (**Mountain_df.te **). 

```{r, echo = FALSE, message = FALSE, warning = FALSE,}
set.seed(123) ## for replication purpose

# the index of the rows that will be in the training set
index.tr <- sample(1:nrow(Mountain_df), replace=FALSE,
                   size=0.75*nrow(Mountain_df))

Mountain_df.tr_notsubs <- Mountain_df[index.tr,] ## the training set
Mountain_df.te <- Mountain_df[-index.tr,] ## the test set
```

### Bootstrap
We can now proceed to the bootstraping with 100 replicates

```{r, echo = FALSE, message = FALSE, warning = FALSE,}
set.seed(897)
index.boot <- createResample(y=Mountain_df.tr_notsubs$Mountain_range, times=100)
head(index.boot[[1]])
tail(index.boot[[1]])
```

```{r, echo = FALSE, message = FALSE, warning = FALSE,}
df.boot.tr <- Mountain_df.tr_notsubs[index.boot[[1]],]
dim(df.boot.tr)

df.boot.val <- Mountain_df.tr_notsubs[-index.boot[[1]],]
dim(df.boot.val)
```

We see that the training set **df.boot.tr** has 205 observations and 28 features and that the validation test **df.boot.val** has 73 observations and 28 features. 

```{r, echo = FALSE, message = FALSE, warning = FALSE,}
# The count of the bootstraped df
ggplot(df.boot.tr) +
  geom_bar(aes(x = Mountain_range))
```

The above plot shows the distribution of the **df.boot.tr**'s instances through the 3 mountains after the bootstraping. 

We can now replicate the models. 

### Neural Network Model (with Unique bootstraped data)
Simple hyperparameter tuning, this code takes time to run.

```{r cache = TRUE, message = FALSE, warning = FALSE, results = 'hide'}
set.seed(1)

nnetFit_boot <- train(Mountain_range ~ ., 
                 data = df.boot.tr,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

```

```{r, echo = FALSE, message = FALSE, warning = FALSE,}
plot(nnetFit_boot)
```

The best neural network should have 4 hidden units and a decay of 0.1. It is the same as before.

```{r, echo = FALSE, message = FALSE, warning = FALSE,}
set.seed(345)
nn4_boot <- nnet(Mountain_range ~ ., data=df.boot.tr, size=4, decay = 0.3)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE,}
nn4_pred_boot <- predict(nn4_boot, df.boot.val, type="class")
```

Now that we fitted the model and made the prediction, we can use the confusion matrix to see is the model performed well or not. 

```{r, echo = FALSE, message = FALSE, warning = FALSE,}
confusionMatrix(data=as.factor(nn4_pred_boot), 
                reference = df.boot.val$Mountain_range)
```

### KNN model (with Unique bootstraped data)

```{r, echo = FALSE, message = FALSE, warning = FALSE,}
set.seed(123)
KNN2 <- knn3(data=df.boot.tr,df.boot.tr$Mountain_range ~ ., k=2)
MR2.te.pred <- predict(KNN2, newdata = df.boot.val,type ="class") 

TAB2 <- table(Obs=df.boot.val$Mountain_range, Pred=MR2.te.pred) # confusion matrix 
TAB2
(ACC <- sum(diag(TAB2))/sum(TAB2)) # accuracy 
```
Now, the accuracy is of 95.89%, lower than before, but might be more realistic with the unique method and the bootstrap. 


--------------------------------


## Unsupervised learning method

### Cluster Analysis 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# New Dataframe: Select only the numerical values:
Mountain_data.num <- select_if(Mountain_data_cleaned, is.numeric)
to.delet <- c('Day', 'Month', 'Year')
Mountain_data.num <- select(Mountain_data.num, -to.delet)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
row.names(Mountain_data_cleaned) <- paste("M", c(1:nrow(Mountain_data_cleaned)), sep="") # row names are used after

# scaling the data 
Mountain_data_cleaned[,-c(1:9)] <- scale(Mountain_data_cleaned[,-c(1:9)])
```


#### Agglomerative Hierarchical Clustering (AGNES) with Manhattan distance

```{r, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
set.seed(123)

# matrix of Manhattan distances 
mountain.d <- dist(Mountain_data_cleaned[,-c(1:9)], method = "manhattan") 

# create a data frame of the distances in long format
mountain.melt <- melt(as.matrix(mountain.d))

ggplot(data = mountain.melt, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() 

#dendrogram using a complete linkage.
mountain.hc <- hclust(mountain.d, method = "complete")
plot(mountain.hc, hang=-1)

#cut the tree to 4 clusters
plot(mountain.hc, hang=-1)
rect.hclust(mountain.hc, k=4)
mountain.clust <- cutree(mountain.hc, k=5)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.width=9,fig.height=10, out.width = "70%", fig.align="center"}
#Interpretation of the clusters
mountain.comp <- data.frame(Mountain_data.num, 
                            Clust=factor(mountain.clust)
                            ,Id=row.names(Mountain_data_cleaned))

mountain.df <- melt(mountain.comp, id=c("Id", "Clust"))
head(mountain.df)

ggplot(mountain.df, aes(y=value, group=Clust, fill=Clust)) +
  geom_boxplot() +
  facet_wrap(~variable, scale="free")
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, out.width = "50%", fig.align="center"}
#number of clusters
fviz_nbclust(Mountain_data_cleaned[,-c(1:9)],
             hcut, hc_method="complete",
             hc_metric="manhattan",
             method = "silhouette", 
             k.max = 25, verbose = FALSE)

mountain.pam <- pam(Mountain_data_cleaned[,-c(1:9)], k=5)
mountain.pam
plot(silhouette(mountain.pam),  col=1:5, border=NA)
```

Using the dendrogram with complete linkage on Manhattan distance with the silhouette method, we identified k=5 as the optimal number of clusters. 

By analyzing this clusters, we can differentiate some of them. 
Firstly, cluster 1 seems to be more or less in the average of the other clusters for all variables, except for its pH, (*pH_B*, *pH_P* and *pH_T*) which is well below the others. 
Cluster 2 is distinguished by its high *PT_P*, *PT_T* and *PT_B*.
Cluster 3 is characterized by a high *pH_B*, *pH_P* and *pH_T* and a low β-glucosidase enzyme (*Glu_B*, *Glu_P* and *Glu_T*) content (like cluster 5). 
By looking at the distribution of cluster 4 we see that it contains few observations but that it is well distinguished from the others. Indeed, only one line appears where the median merges with the width of its distribution. This cluster has the lowest *radiation*, the highest soil organic carbon (*SOC_B*, *SOC_T* and *SOC_P*) and highest soil total nitrogen (*NT_T*, *NT_B* and *NT_P*). 
Finally, the last cluster has a pH (*pH_B*, *pH_P* and *pH_T*) close to zero and a high *radiation*. It has oservations with the lowest soil organic carbon (*SOC_B*, *SOC_T* and *SOC_P*)

