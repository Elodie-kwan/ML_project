---
output:
  html_document: default
  pdf_document: default
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
source("../scripts/setup.R")
```

## Part 3 : Analysis 

```{r include = FALSE}
Mountain_data_cleaned <- read.csv("../data/Mountain_data_cleaned.csv")
```

```{r , echo = FALSE, message = FALSE, warning = FALSE}
Mountain_data_cleaned$Country <- as.factor(Mountain_data_cleaned$Country)
Mountain_data_cleaned$Mountain_range <- as.factor(Mountain_data_cleaned$Mountain_range)
Mountain_data_cleaned$Locality <- as.factor(Mountain_data_cleaned$Locality)
Mountain_data_cleaned$Plot <- as.factor(Mountain_data_cleaned$Plot)
Mountain_data_cleaned$Subplot <- as.factor(Mountain_data_cleaned$Subplot)

Mountain_data_cleaned$Date <- as.Date(Mountain_data_cleaned$Date)
```

### Replace the NAs by the mean of the closest observations
Before starting with the analysis we know that some of the models we use do not work with NAs. To deal with them, we decided to replace the 2 NAs we have by the mean of their closest observations. 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
mean_for_fill <-colMeans( Mountain_data_cleaned %>%
  select(Plot,Glu_P)%>%
  filter(Plot %in% c(76, 77))%>%na.omit()%>% select(Glu_P))
  
Mountain_data_cleaned[is.na(Mountain_data_cleaned)] <- mean_for_fill
rm(mean_for_fill)
```

### Splitting the data into Traning set and Test set 
The next step is to split our data set into a training set (**Mountain_data.tr_notsubs**) and a test set (**Mountain_data.te**). The training set has 75% of the observations and the test set has the 25% remainder.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(123) ## for replication purpose

## the index of the rows that will be in the training set
index.tr <- sample(1:nrow(Mountain_data_cleaned), replace=FALSE,
                   size=0.75*nrow(Mountain_data_cleaned))

Mountain_data.tr_notsubs <- Mountain_data_cleaned[index.tr,] ## the training set
Mountain_data.te <- Mountain_data_cleaned[-index.tr,] ## the test set
```

### Balancing the Training set 
As discussed in the EDA part, we should balance our data because we do not have the same amount of information on each mountains. We have more observations on **Sierra de Guadarrama** and half less on **Central Andes**. 

We balanced the data on the training set **Mountain_data.tr_notsubs**.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(123)
no.mountain_1 <- min(table(Mountain_data.tr_notsubs$Mountain_range)) ## 79

## the "Central Andes" cases
data.tr.mountain_1 <- filter(Mountain_data.tr_notsubs, Mountain_range=="Central Andes")

## the "Central Pyrenees" cases
data.tr.mountain_2 <- filter(Mountain_data.tr_notsubs, Mountain_range=="Central Pyrenees")

## The "Sierra de Guadarrama" cases 
data.tr.mountain_3 <- filter(Mountain_data.tr_notsubs, Mountain_range=="Sierra de Guadarrama") 
## sub-sample 79 instances from the number of "Central Pyrenees" cases
index.mountain_2 <- sample(size=no.mountain_1, 
                           x=1:nrow(data.tr.mountain_2), 
                           replace=FALSE)

## sub-sample 79 instances from the number of "Sierra de Guadarrama" cases
index.mountain_3 <- sample(size=no.mountain_1, 
                           x=1:nrow(data.tr.mountain_3), 
                           replace=FALSE)
  
## Bind all the "Central Andes" and the sub-sampled "Central Pyrenees" 
## and the sub-sampled "Sierra de Guadarrama"
Mountain_data.tr <- data.frame(rbind(data.tr.mountain_1,
                                     data.tr.mountain_2[index.mountain_2,],
                                     data.tr.mountain_3[index.mountain_3,])) 

## The cases are now balanced
table(Mountain_data.tr$Mountain_range)
```

Now we see that all three mountains has the same amount of observations. We decided to sub-samples the observations because we think that duplicating them will create more biases as we do not have that many observations. 

Our new training set is **Mountain_data.tr**. 

### Neural Network Model 

We start by fitting a Neural Network model on our balanced training set (**Mountain_data.tr**). In order to choose the parameters of this neural network, we applied a simple hyperparameter tuning. 

```{r cache = TRUE, message = FALSE, warning = FALSE, results = 'hide', echo=FALSE}
# this code takes time to run
set.seed(1)
fitControl <- trainControl(method = "cv", 
                           number = 10)

nnetGrid <-  expand.grid(size = seq(from = 1, to = 6, by = 1),
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))

nnetFit <- train(Mountain_range ~ ., 
                 data = Mountain_data.tr,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
plot(nnetFit)
```

The best Neural Networks parameters would be to choose 4 hidden layers, with a decay of 0.1 since it is the best combination that gives the highest accuracy value. 

The manually written Neural Network model 
```{r, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(345)
nn4 <- nnet(Mountain_range ~ ., data=Mountain_data.tr, size=4, decay = 0.1)

nn4_pred <- predict(nn4, Mountain_data.te, type="class")
```

### Accuracy of Neural Network
```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Confusion Matrix
confusionMatrix(data=as.factor(nn4_pred), reference = Mountain_data.te$Mountain_range)
```

With this confusion matrix command (*confusionMatrix*), we have more information on the model. As said before, we see that the accuracy is very high (99%) and we also see that we have a balanced accuracy of 1 which is the maximum we can get and which mean that our model do not suffer from unbalanced data. 




### Naive Bayes 

```{r, echo = FALSE, message = FALSE, warning = FALSE, figures-side1, fig.show="hold" ,out.width="50%"}

d1_ph <- Mountain_data_cleaned %>% ggplot(aes(x =pH_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

d2_ph <- Mountain_data_cleaned %>% ggplot(aes(x =pH_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

d3_ph <- Mountain_data_cleaned %>% ggplot(aes(x =pH_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)


d1_phos <- Mountain_data_cleaned %>% ggplot(aes(x =Phos_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

d2_phos <- Mountain_data_cleaned %>% ggplot(aes(x =Phos_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

d3_phos <- Mountain_data_cleaned %>% ggplot(aes(x =Phos_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

(d1_phos / d2_phos / d3_phos+ plot_annotation(title = 'Phosphatase enzyme'))

(d1_ph / d2_ph / d3_ph + plot_annotation(title = 'PH'))
```


```{r, echo = FALSE, message = FALSE, warning = FALSE}

d1_glu <- Mountain_data_cleaned %>% ggplot(aes(x =Glu_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

d2_glu <- Mountain_data_cleaned %>% ggplot(aes(x =Glu_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

d3_glu <- Mountain_data_cleaned %>% ggplot(aes(x =Glu_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)


```


```{r, echo = FALSE, message = FALSE, warning = FALSE, figures-side2, fig.show="hold", out.width="50%"}

d1_soc <- Mountain_data_cleaned %>% ggplot(aes(x =SOC_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

d2_soc <- Mountain_data_cleaned %>% ggplot(aes(x =SOC_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

d3_soc <- Mountain_data_cleaned %>% ggplot(aes(x =SOC_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

(d1_glu / d2_glu / d3_glu+ plot_annotation( title = 'Î²-glucosidase enzyme'))
(d1_soc / d2_soc / d3_soc+ plot_annotation(title = 'soil organic carbon'))



```


```{r, echo = FALSE, message = FALSE, warning = FALSE}

d1_pt <- Mountain_data_cleaned %>% ggplot(aes(x =PT_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

d2_pt <- Mountain_data_cleaned %>% ggplot(aes(x =PT_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

d3_pt <- Mountain_data_cleaned %>% ggplot(aes(x =PT_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)



```


```{r, echo = FALSE, message = FALSE, warning = FALSE, figures-side3, fig.show="hold", out.width="50%"}
d1_k <- Mountain_data_cleaned %>% ggplot(aes(x =K_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

d2_k <- Mountain_data_cleaned %>% ggplot(aes(x =K_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

d3_k <- Mountain_data_cleaned %>% ggplot(aes(x =K_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)


d1_pt / d2_pt / d3_pt + plot_annotation(
  title = 'available phosphorus')

d1_k / d2_k / d3_k + plot_annotation(
  title = 'potassium content')
```


```{r, echo = FALSE, message = FALSE, warning = FALSE}

d1_cond <- Mountain_data_cleaned %>% ggplot(aes(x =Cond_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

d2_cond <- Mountain_data_cleaned %>% ggplot(aes(x =Cond_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

d3_cond <- Mountain_data_cleaned %>% ggplot(aes(x =Cond_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)


```


```{r, echo = FALSE, message = FALSE, warning = FALSE, figures-side4, fig.show="hold", out.width="50%"}
d1_nt <- Mountain_data_cleaned %>% ggplot(aes(x =NT_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

d2_nt <- Mountain_data_cleaned %>% ggplot(aes(x =NT_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

d3_nt <- Mountain_data_cleaned %>% ggplot(aes(x =NT_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)


d1_cond / d2_cond / d3_cond+ plot_annotation(
  title = 'electrical conductivity')

d1_nt / d2_nt / d3_nt+ plot_annotation(
  title = 'soil total nitrogen')
```


```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.align='center', out.width="60%"}
d1_radiation <- Mountain_data_cleaned %>% ggplot(aes(x =Radiation, fill= `Mountain_range`))+geom_density(alpha = 0.3)+labs(title = "Radiation")



d1_radiation
```

The analysis of the "Naive Bayes" model comes back to the analysis of the density graphs of each variable by mountain range.
From these density graphs we agree on what has been seen before, that the *pH* is a very good feature to classify the mountains. Indeed, *pH_T* *ph_B* and *ph_P* is medium in Central Andes, lower in Sierra de Guadarrama and higher in Central Pyrenees. 

In addition to that, the observation of other variables such as phosphatase enzyme, (*Phos_P, Phos_B, Phos_T*), Î²-glucosidase enzyme (*Glu_B*, *Glu_P*, *Glu_T*) , soil organic carbon (*SOC_T*, *SOC_B*, *SOC_P*), soil total nitrogen (*NT_P*,*NT_B*,*NT_T*), electrical conductivity (*Cond_P*,*Cond_B*, *Cond_T*), and the *radiation* could allow us to get an idea of the mountain we are on.
Indeed, a radiation lower than 0,6 indicates rather the Central Pyrenees. The electrical conductivity allows us to distinguish the Central Andes from the Central Pyrenees, since it is higher for the latter. 
If we look at the soil total nitrogen, we see that it is lower for the Central Andes than for the two others. 
The soil organic carbon allows us to distinguish the Central Andes from the Sierra de Guadarrama, since it is higher for this last one.
The Î²-glucosidase enzyme is present at approximately the same level in Central Andes and Central Pyrenees but has a higher value in Sierra de Guadarrama. It is about the same for the phosphatase enzyme 

However, some variables do not allow us to determine the mountain at all. It is the case by observing the phosphorus (*PT_P, PT_B, PT_T*) or the potassium content (*K_P, K_B, K_T*).

### Accuracy of Naive bayes
```{r echo = FALSE, warning=FALSE, message=FALSE}
mountain.nb <- naive_bayes(Mountain_range ~. ,data =Mountain_data.tr[,c(2,10:24)])
nb_train_predict <- predict(mountain.nb, 
                            Mountain_data.te[ , 
                                              names(Mountain_data.te)
                                              !="Mountain_range"])
cfm_nb <- confusionMatrix(nb_train_predict, Mountain_data.te$Mountain_range)
cfm_nb                          
```

Through this confusion matrix, we can see that the Naive Bayes accuracy is of 96.3% and the balancced accuracy of 90.48%. Indeed it lower than the Neural Network, but it remain good. 3 Central Pyrenees have been mispredicted, and 1 Sierra de Guadarrama. 


### K-NN Model

We use a 2-NN to predict the test set using the training set

```{r, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(123)
KNN <- knn3(data=Mountain_data.tr,Mountain_data.tr$Mountain_range ~ ., k=2)
MR.te.pred <- predict(KNN, newdata = Mountain_data.te,type ="class") 

# confusion matrix accuracy 

confusionMatrix(MR.te.pred, Mountain_data.te$Mountain_range)
```

With this KNN supervised method, the prediction made on the test set gives us an accuracy of 1. It looks indeed perfect and attractive, but it is complytely biased by the fact that subplot can be identical to plot. Therefore, there is an overlay in the pbservations and the distance computed is 0. What we need to do is to make a unite dataset with no observations overlayed.


### Analysis with unique dataset 

We now want to replicate the above models with a another training set and test set that has been through a boostraping method. To do so, we first want to delete all the repeated observations from the original cleaned dataset. 

#### Unique

We remove every replicated data with the "unique" function. We decided to do so to see if we will have better results. As we see above, the accuracy of our models are very high and this is normally unlikely. So we are testing if the high accuracy is due to the fact that we have duplicated observations in our dataset. (The scientists did sample different observations, but it comes out that the samples are exactly the same, they are maybe different at a very very small digits.)


```{r, echo = FALSE, message = FALSE, warning = FALSE}
Mountain_df <- Mountain_data_cleaned[-c(4:9)]
Mountain_df <- unique(Mountain_df)
```

#### Count

We have seen that the new Unique data set is largely unbalanced. The mountain that has the most information is now the one with the less observations, so information.


Sierra de Gaudarrama, after removing every replicated data with the "unique" function, only have 39 observations. It is unbalanced indeed, and insufficient to make accurate classification methods. 


#### Split in a new training set and test set

Now that we have a new dataset, we have to split it again into a training set (**Mountain_df.tr_notsubs**) and a test set (**Mountain_df.te **). 

```{r, echo = FALSE, message = FALSE, warning = FALSE,}
set.seed(123) ## for replication purpose

# the index of the rows that will be in the training set
index.tr <- sample(1:nrow(Mountain_df), replace=FALSE,
                   size=0.75*nrow(Mountain_df))

Mountain_df.tr_notsubs <- Mountain_df[index.tr,] ## the training set
Mountain_df.te <- Mountain_df[-index.tr,] ## the test set
```

### Random Forest 

Once the duplicates are removed, we are left with a data set that is low in data, especially in the observations for the **Sierra de Guadarrama** class.  With the poverty of our data we want to test an implementation of a random forest model. Indeed, the bagging should allow us to overcome the lack of data and give the best possible model. Another advantage of the random forest model is that it allows us to easily identify the importance of variables.

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.align='center', out.width= "70%"}
train_set_for_RF <- Mountain_df.tr_notsubs %>%
  select(!c( Locality, Country))
test_set_for_RF <- Mountain_df.te%>%
  select(!c( Locality, Country))

Random_Forest_full_variable <- randomForest( Mountain_range ~ .,data=train_set_for_RF, importance = TRUE)
predtr <- predict(Random_Forest_full_variable, newdata = train_set_for_RF[-1])
predte <-  predict(Random_Forest_full_variable, newdata=test_set_for_RF[-1])
cmtr_rf <-   table(train_set_for_RF[,1], predtr)      
cmte_rf <-  table(test_set_for_RF[,1], predte)

varImpPlot(Random_Forest_full_variable, cex = 0.65)

#importance(rf)
Acc_tr_rf <- sum(diag(cmtr_rf))/sum(cmtr_rf)
Acc_te_rf <- sum(diag(cmte_rf))/sum(cmte_rf)

```

Here we can see the difference between the descending average of the precision that each variable adds to the model and the descending average of Gini which is a coefficient that shows the contribution of each variable to the homogeneity of nodes and leaves. We notice that some variables have a higher precision importance than their importance based on their Gini score, for our selection of future candidate variables for a simpler model we will make a compromise between these two measures. 

From the variable importance we see that the variable **pH_P**, **pH_T** and **pH_B** are very important. We can then say that the *pH* in general is important. 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Confusion Matrix of the Random Forest model on training set 
cmtr_rf
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
cat("Accuracy of the train set =",Acc_tr_rf)
```
The first table is the confusion matrix of the Random Forest model on the training set. Therefore it is normal to have an accuracy of 1 since the model already knows the observations. 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Confusion Matrix of the Random Forest model on test set
cmte_rf
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
cat("Accuracy of the test set =", Acc_te_rf)
```
Then the second table is the confusion matrix of the random forest model on the test set.  We still have a precision of 1, which makes it a good model. We have to keep in mind that the data set is very small and not necessarily representative of a larger sample.  

Currently the model has many variables and we notice that many of them are not important for mountain classification. We are therefore thinking of testing a model with the 5 main variables to avoid overloading the model. Having less variables would allow us to predict a larger set of data with less computational power and resources. Moreover, the time saving could also be done during sampling, since less chemical elements would be needed to predict the class. 


```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.align='center', out.width= "70%"}
train_set_for_RF <- Mountain_df.tr_notsubs %>%
  select(c( Mountain_range, pH_P, pH_B,pH_T, NT_B, Cond_B))
test_set_for_RF <- Mountain_df.te%>%
  select(c(Mountain_range, pH_P, pH_B,pH_T, NT_B, Cond_B))

Random_Forest_selected_variable <- randomForest( Mountain_range ~ .,data=train_set_for_RF, importance = TRUE)
predtr <- predict(Random_Forest_selected_variable, newdata = train_set_for_RF[-1])
predte <-  predict(Random_Forest_selected_variable, newdata=test_set_for_RF[-1])
cmtr_rf2 <-   table(train_set_for_RF[,1], predtr)      
cmte_rf2 <-  table(test_set_for_RF[,1], predte)

varImpPlot(Random_Forest_selected_variable, cex = 0.65)
#importance(rf)
Acc_tr_rf2 <- sum(diag(cmtr_rf2))/sum(cmtr_rf2)
Acc_te_rf2 <- sum(diag(cmte_rf2))/sum(cmte_rf2)

```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Confusion Matrix of the Random Forest model on training set 
cmtr_rf2
```


```{r, echo = FALSE, message = FALSE, warning = FALSE}
cat("Accuracy of the train set =",Acc_tr_rf2)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Confusion Matrix of the Random Forest model on training set 
cmte_rf2
```


```{r, echo = FALSE, message = FALSE, warning = FALSE}
cat("Accuracy of the test set =", Acc_te_rf2)
```
Even with this model composed of fewer variables we still see a good accuracy of the test set compared to the train set. Here the error comes from the fact that we will have more tendency to predict **Central Andes** since we cannot balance the data between the classes otherwise we will reduce too much the size of the data set. 

Overall the random forest model seems to be a good model that can be used with fewer variables while keeping a high accuracy. 

But we now bootstrap to re-test some of our models and check if the accuracy changes once the duplicate data is removed. 

#### Bootstrap
We can now proceed to the bootstraping with 100 replicates

```{r message=FALSE, warning=FALSE, include=FALSE}
set.seed(897)
index.boot <- createResample(y=Mountain_df.tr_notsubs$Mountain_range, times=100)
head(index.boot[[1]])
tail(index.boot[[1]])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
df.boot.tr <- Mountain_df.tr_notsubs[index.boot[[1]],]
dim(df.boot.tr)

df.boot.val <- Mountain_df.tr_notsubs[-index.boot[[1]],]
dim(df.boot.val)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, out.width="50%", figures-side_last, fig.show="hold", }
ggplot(Mountain_df, aes(x = Mountain_range,)) +
  geom_bar(aes( fill = Mountain_range))+ 
  labs(title ="Proportion of observations before bootstrap", subtitle = "Without duplicate data")+ theme(plot.title = element_text(size=20))+
  geom_text(stat='count', aes(label=..count..), vjust=-0.2)


ggplot(df.boot.tr, aes(x = Mountain_range,)) +
  geom_bar(aes( fill= Mountain_range))+ 
  labs(title ="Proportion of observations after bootstrap", subtitle = "Without duplicate data")+ theme(plot.title = element_text(size=20))  +
  geom_text(stat='count', aes(label=..count..), vjust=-0.2)

  
```


We can now replicate the models.  

### Neural Network Model (with Unique bootstraped data)
Simple hyperparameter tuning, this code takes time to run.

```{r cache = TRUE, message = FALSE, warning = FALSE, results = 'hide', echo=FALSE}
set.seed(1)

nnetFit_boot <- train(Mountain_range ~ ., 
                 data = df.boot.tr,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

```

```{r, echo = FALSE, message = FALSE, warning = FALSE,}
plot(nnetFit_boot)
```

The best neural network should have 4 hidden units and a decay of 0.1. It is the same as before.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(345)
nn4_boot <- nnet(Mountain_range ~ ., data=df.boot.tr, size=4, decay = 0.3)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE,}
nn4_pred_boot <- predict(nn4_boot, df.boot.val, type="class")
```

Now that we fitted the model and made the prediction, we can use the confusion matrix to see is the model performed well or not. 

```{r, echo = FALSE, message = FALSE, warning = FALSE,}
confusionMatrix(data=as.factor(nn4_pred_boot), 
                reference = df.boot.val$Mountain_range)
```

### KNN model (with Unique bootstraped data)

```{r, echo = FALSE, message = FALSE, warning = FALSE,}
set.seed(123)
KNN2 <- knn3(data=df.boot.tr,df.boot.tr$Mountain_range ~ ., k=2)
MR2.te.pred <- predict(KNN2, newdata = df.boot.val,type ="class") 

# confusion matrix / accuracy 
confusionMatrix(MR2.te.pred,df.boot.val$Mountain_range)
```
Now, the accuracy is of 95.89%, lower than before, but might be more realistic with the unique method and the bootstrap. The balanced accuracy remains equal to 1.


--------------------------------


### Unsupervised learning method

#### Cluster Analysis 

```{r echo=FALSE, results='hide',message=FALSE, warning = FALSE}
# New Dataframe: Select only the numerical values:
Mountain_data.num <- select_if(Mountain_data_cleaned, is.numeric)
to.delet <- c('Day', 'Month', 'Year')
Mountain_data.num <- select(Mountain_data.num, -to.delet)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
row.names(Mountain_data_cleaned) <- paste("M", c(1:nrow(Mountain_data_cleaned)), sep="") # row names are used after

# scaling the data 
Mountain_data_cleaned[,-c(1:9)] <- scale(Mountain_data_cleaned[,-c(1:9)])
```


#### Agglomerative Hierarchical Clustering (AGNES) with Manhattan distance

```{r echo=FALSE, results='hide',message=FALSE, warning = FALSE, fig.show='hide'}
set.seed(123)

# matrix of Manhattan distances 
mountain.d <- dist(Mountain_data_cleaned[,-c(1:9)], method = "manhattan") 

# create a data frame of the distances in long format
mountain.melt <- melt(as.matrix(mountain.d))

ggplot(data = mountain.melt, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() 

#dendrogram using a complete linkage.
mountain.hc <- hclust(mountain.d, method = "complete")
plot(mountain.hc, hang=-1)

#cut the tree to 4 clusters
plot(mountain.hc, hang=-1)
rect.hclust(mountain.hc, k=4)
mountain.clust <- cutree(mountain.hc, k=5)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.width=13,fig.height=10, out.width = "70%", fig.align="center"}
#Interpretation of the clusters
mountain.comp <- data.frame(Mountain_data.num, 
                            Clust=factor(mountain.clust)
                            ,Id=row.names(Mountain_data_cleaned))

mountain.df <- melt(mountain.comp, id=c("Id", "Clust"))

ggplot(mountain.df, aes(y=value, group=Clust, fill=Clust)) +
  geom_boxplot() +
  facet_wrap(~variable, scale="free")
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, out.width = "70%", fig.align="center", fig.width=10,fig.height=4}
#number of clusters
fviz_nbclust(Mountain_data_cleaned[,-c(1:9)],
             hcut, hc_method="complete",
             hc_metric="manhattan",
             method = "silhouette", 
             k.max = 25, verbose = FALSE)

```

```{r, echo = FALSE, message = FALSE, warning = FALSE, out.width = "100%", fig.align="center", fig.width=10,fig.height=6,}
#number of clusters

mountain.pam <- pam(Mountain_data_cleaned[,-c(1:9)], k=5)

plot(silhouette(mountain.pam),  col=1:5, border=NA, main="Silhouette plot of pam")
```

Using the dendrogram with complete linkage on Manhattan distance with the silhouette method, we identified k=5 as the optimal number of clusters. 

By analyzing this clusters, we can differentiate some of them. 
Firstly, cluster 1 seems to be more or less in the average of the other clusters for all variables, except for its pH, (*pH_B*, *pH_P* and *pH_T*) which is well below the others. 
Cluster 2 is distinguished by its high *PT_P*, *PT_T* and *PT_B*.
Cluster 3 is characterized by a high *pH_B*, *pH_P* and *pH_T* and a low Î²-glucosidase enzyme (*Glu_B*, *Glu_P* and *Glu_T*) content (like cluster 5). 
By looking at the distribution of cluster 4 we see that it contains few observations but that it is well distinguished from the others. Indeed, only one line appears where the median merges with the width of its distribution. This cluster has the lowest *radiation*, the highest soil organic carbon (*SOC_B*, *SOC_T* and *SOC_P*) and highest soil total nitrogen (*NT_T*, *NT_B* and *NT_P*). 
Finally, the last cluster has a pH (*pH_B*, *pH_P* and *pH_T*) close to zero and a high *radiation*. It has oservations with the lowest soil organic carbon (*SOC_B*, *SOC_T* and *SOC_P*)
On the silhouette plot, we see that the average silhouette considering the all five clusters is 0.45. Cluster 1 and 4 are the most homogeneous clusters and with the highest silhouette, respectively, 0,56 and 0,70. Cluster 5 has some mountains badly clustered as it has negative silhouettes, and the lowest silhouette (0,15).

To conclude, we will do a final score analysis of each models, so we can select the best one. 








