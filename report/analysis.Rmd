---
output:
  html_document: default
  pdf_document: default
---

```{r, echo = FALSE, message = FALSE}
source("../scripts/setup.R")
```

## Part 3 : Ananlysis 

### Import dataset
Let's import the cleaned dataset that we created. 

```{r}
Mountain_data_cleaned <- read.csv("../data/Mountain_data_cleaned.csv")
```

```{r echo=FALSE}
Mountain_data_cleaned$Country <- as.factor(Mountain_data_cleaned$Country)
Mountain_data_cleaned$Mountain_range <- as.factor(Mountain_data_cleaned$Mountain_range)
Mountain_data_cleaned$Locality <- as.factor(Mountain_data_cleaned$Locality)
Mountain_data_cleaned$Plot <- as.factor(Mountain_data_cleaned$Plot)
Mountain_data_cleaned$Subplot <- as.factor(Mountain_data_cleaned$Subplot)

Mountain_data_cleaned$Date <- as.Date(Mountain_data_cleaned$Date)
```

### Replace the NAs by the mean of the closest observations
Some of the models we use do not work with NAs. To deal with them, we decided to replace the 2 NAs we have by the mean of their closest observations. 

```{r}
mean_for_fill <-colMeans( Mountain_data_cleaned %>%
  select(Plot,Glu_P)%>%
  filter(Plot %in% c(76, 77))%>%na.omit()%>% select(Glu_P))
  
Mountain_data_cleaned[is.na(Mountain_data_cleaned)] <- mean_for_fill
rm(mean_for_fill)
```

### Splitting the data into Traning set and Test set 

```{r}
set.seed(123) ## for replication purpose

## the index of the rows that will be in the training set
index.tr <- sample(1:nrow(Mountain_data_cleaned), replace=FALSE,
                   size=0.75*nrow(Mountain_data_cleaned))

Mountain_data.tr_notsubs <- Mountain_data_cleaned[index.tr,] ## the training set
Mountain_data.te <- Mountain_data_cleaned[-index.tr,] ## the test set
```

### Balancing the Training set 
As discussed in the EDA part, we should balance our data because we do not have the same amount of information on each mountains. We have more observations on **Sierra de Guadarrama** and half less on **Central Andes**. 

```{r}
no.mountain_1 <- min(table(Mountain_data.tr_notsubs$Mountain_range)) ## 79

## the "Central Andes" cases
data.tr.mountain_1 <- filter(Mountain_data.tr_notsubs, Mountain_range=="Central Andes")

## the "Central Pyrenees" cases
data.tr.mountain_2 <- filter(Mountain_data.tr_notsubs, Mountain_range=="Central Pyrenees")

## The "Sierra de Guadarrama" cases 
data.tr.mountain_3 <- filter(Mountain_data.tr_notsubs, Mountain_range=="Sierra de Guadarrama") 
## sub-sample 79 instances from the number of "Central Pyrenees" cases
index.mountain_2 <- sample(size=no.mountain_1, 
                           x=1:nrow(data.tr.mountain_2), 
                           replace=FALSE)

## sub-sample 79 instances from the number of "Sierra de Guadarrama" cases
index.mountain_3 <- sample(size=no.mountain_1, 
                           x=1:nrow(data.tr.mountain_3), 
                           replace=FALSE)
  
## Bind all the "Central Andes" and the sub-sampled "Central Pyrenees" 
## and the sub-sampled "Sierra de Guadarrama"
Mountain_data.tr <- data.frame(rbind(data.tr.mountain_1,
                                     data.tr.mountain_2[index.mountain_2,],
                                     data.tr.mountain_3[index.mountain_3,])) 

## The cases are now balanced
table(Mountain_data.tr$Mountain_range)
```


### Neural Network Model 
Simple hyperparameter tuning, this code takes time to run.

```{r cache = TRUE, message = FALSE, warning = FALSE, results = 'hide'}
set.seed(1)
fitControl <- trainControl(method = "cv", 
                           number = 10)

nnetGrid <-  expand.grid(size = seq(from = 1, to = 6, by = 1),
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))

nnetFit <- train(Mountain_range ~ ., 
                 data = Mountain_data.tr,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

```

```{r}
plot(nnetFit)
```

The best Neural Networks parameters would be to choose 4 hidden layers, with a decay of 0.1. 

The manually written Neural Network model 
```{r}
set.seed(345)
nn4 <- nnet(Mountain_range ~ ., data=Mountain_data.tr, size=4, decay = 0.1)

nn4_pred <- predict(nn4, Mountain_data.te, type="class")
tab4 <- table(Obs=Mountain_data.te$Mountain_range, Pred=nn4_pred) # confusion matrix
tab4
(acc4 <- sum(diag(tab4))/sum(tab4)) # accuracy

```

Here it says that it has almost perfect accuracy (99%).

```{r}
# Confusion Matrix
confusionMatrix(data=as.factor(nn4_pred), reference = Mountain_data.te$Mountain_range)
```

With this confusion matrix command we have more information on the model. As said before, we see that the accuracy is very high (99%) and we also see that we have a balanced accuracy of 1 which is the maximum we can get and which mean that our model do not suffer from unbalanced data. 


### Random Forest 
```{r}
train_set_for_RF <- Mountain_data.tr %>%
  select(!c(Plot, Subplot, Date, Day, Month, Year, Locality, Country))
test_set_for_RF <- Mountain_data.te%>%
  select(!c(Plot, Subplot, Date, Day, Month, Year, Locality, Country))

rf <- randomForest( Mountain_range ~ .,data=train_set_for_RF, importance = TRUE)
predtr <- predict(rf, newdata = train_set_for_RF[-1])
predte <-  predict(rf, newdata=test_set_for_RF[-1])
cmtr <-   table(train_set_for_RF[,1], predtr)      
cmte <-  table(test_set_for_RF[,1], predte)

varImpPlot(rf, cex = 0.65)
importance(rf)
Acc_tr <- sum(diag(cmtr))/sum(cmtr)
Acc_te <- sum(diag(cmte))/sum(cmte)

```

From the variable importance we see that the variable **pH_P**, **pH_T** and **pH_B** are very important. We can then say that the *pH* in general is important. 

```{r}
# Confusion Matrix of the Random Forest model on training set 
cmtr
Acc_tr

# Confusion Matrix of the Random Forest model on training set 
cmte
Acc_te
```

We see that the model has a accuracy of 99% when it comes to predict new observations. 

### Naive Bayes 

#### PH
```{r}

d1 <- Mountain_data_cleaned %>% ggplot(aes(x =pH_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

d2 <- Mountain_data_cleaned %>% ggplot(aes(x =pH_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

d3 <- Mountain_data_cleaned %>% ggplot(aes(x =pH_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

d1 / d2 / d3
```
#### Phosphatase enzyme
```{r}
x1 <- Mountain_data_cleaned %>% ggplot(aes(x =Phos_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

x2 <- Mountain_data_cleaned %>% ggplot(aes(x =Phos_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

x3 <- Mountain_data_cleaned %>% ggplot(aes(x =Phos_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

x1 / x2 / x3
```
#### β-glucosidase enzyme
```{r}
y1 <- Mountain_data_cleaned %>% ggplot(aes(x =Glu_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

y2 <- Mountain_data_cleaned %>% ggplot(aes(x =Glu_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

y3 <- Mountain_data_cleaned %>% ggplot(aes(x =Glu_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

y1 / y2 / y3
```
#### soil organic carbon
```{r}
z1 <- Mountain_data_cleaned %>% ggplot(aes(x =SOC_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

z2 <- Mountain_data_cleaned %>% ggplot(aes(x =SOC_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

z3 <- Mountain_data_cleaned %>% ggplot(aes(x =SOC_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

z1 / z2 / z3
```
#### Phosphatase enzyme
```{r}
x1 <- Mountain_data_cleaned %>% ggplot(aes(x =Phos_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

x2 <- Mountain_data_cleaned %>% ggplot(aes(x =Phos_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

x3 <- Mountain_data_cleaned %>% ggplot(aes(x =Phos_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

x1 / x2 / x3
```
#### β-glucosidase enzyme
```{r}
y1 <- Mountain_data_cleaned %>% ggplot(aes(x =Glu_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

y2 <- Mountain_data_cleaned %>% ggplot(aes(x =Glu_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

y3 <- Mountain_data_cleaned %>% ggplot(aes(x =Glu_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

y1 / y2 / y3
```
#### soil organic carbon
```{r}
z1 <- Mountain_data_cleaned %>% ggplot(aes(x =SOC_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

z2 <- Mountain_data_cleaned %>% ggplot(aes(x =SOC_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

z3 <- Mountain_data_cleaned %>% ggplot(aes(x =SOC_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

z1 / z2 / z3
```
#### soil total nitrogen
```{r}
a1 <- Mountain_data_cleaned %>% ggplot(aes(x =NT_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

a2 <- Mountain_data_cleaned %>% ggplot(aes(x =NT_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

a3 <- Mountain_data_cleaned %>% ggplot(aes(x =NT_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

a1 / a2 / a3
```
#### electrical conductivity
```{r}

e1 <- Mountain_data_cleaned %>% ggplot(aes(x =Cond_P, fill= `Mountain_range`))+geom_density(alpha = 0.3) + theme(legend.position="none")

e2 <- Mountain_data_cleaned %>% ggplot(aes(x =Cond_B, fill= `Mountain_range`))+geom_density(alpha = 0.3)+ theme(legend.position="none")

e3 <- Mountain_data_cleaned %>% ggplot(aes(x =Cond_T, fill= `Mountain_range`))+geom_density(alpha = 0.3)

e1 / e2 / e3
```
#### radiation
```{r}
f1 <- Mountain_data_cleaned %>% ggplot(aes(x =Radiation, fill= `Mountain_range`))+geom_density(alpha = 0.3)
f1
```

The analysis of the "Naive Bayes" model comes back to the analysis of the density graphs of each variable by mountain range.
From these density graphs we agree on what has been seen before, that the *pH* is a very good feature to classify the mountains. Indeed, *pH_T* *ph_B* and *ph_P* is medium in Central Andes, lower in Sierra de Guadarrama and higher in Central Pyrenees. 

In addition to that, the observation of other variables such as phosphatase enzyme, (*Phos_P, Phos_B, Phos_T*), β-glucosidase enzyme (*Glu_B*, *Glu_P*, *Glu_T*) , soil organic carbon (*SOC_T*, *SOC_B*, *SOC_P*), soil total nitrogen (*NT_P*,*NT_B*,*NT_T*), electrical conductivity (*Cond_P*,*Cond_B*, *Cond_T*), and the *radiation* could allow us to get an idea of the mountain we are on.
Indeed, a radiation lower than 0,6 indicates rather the Central Pyrenees. The electrical conductivity allows us to distinguish the Central Andes from the Central Pyrenees, since it is higher for the latter. 
If we look at the soil total nitrogen, we see that it is lower for the Central Andes than for the two others. 
The soil organic carbon allows us to distinguish the Central Andes from the Sierra de Guadarrama, since it is higher for this last one.
The β-glucosidase enzyme is present at approximately the same level in Central Andes and Central Pyrenees but has a higher value in Sierra de Guadarrama. It is about the same for the phosphatase enzyme 

However, some variables do not allow us to determine the mountain at all. It is the case by observing the phosphorus (*PT_P, PT_B, PT_T*) or the potassium content (*K_P, K_B, K_T*).


### K-NN Model

We use a 2-NN to predict the test set using the training set

```{r}
set.seed(123)
KNN <- knn3(data=Mountain_data.tr,Mountain_data.tr$Mountain_range ~ ., k=2)
MR.te.pred <- predict(KNN, newdata = Mountain_data.te,type ="class") 

TAB <- table(Obs=Mountain_data.te$Mountain_range, Pred=MR.te.pred) # confusion matrix 
TAB
(ACC <- sum(diag(TAB))/sum(TAB)) # accuracy 
```

----------------------------------------------------------------------------------------------------------------------------------------------------------


### Analysis with unique dataset 

We now want to replicate the above models with a another training set and test set that has been through a boostraping method. To do so, we first want to delete all the repeated observations from the original cleaned dataset. 

#### Unique

We remove every replicated data with the "unique" function. 

```{r}
Mountain_df <- Mountain_data_cleaned[-c(4:9)]
Mountain_df <- unique(Mountain_df)
```

#### Count

```{r}
ggplot(Mountain_df) +
  geom_bar(aes(x = Mountain_range))
```
Here we see the new Unique dataset and notice that it is largely unbalanced. The mountain that has the most information is now the one with the less observations, so information.

#### Split in a new training set and test set

```{r}
set.seed(123) ## for replication purpose

# the index of the rows that will be in the training set
index.tr <- sample(1:nrow(Mountain_df), replace=FALSE,
                   size=0.75*nrow(Mountain_df))

Mountain_df.tr_notsubs <- Mountain_df[index.tr,] ## the training set
Mountain_df.te <- Mountain_df[-index.tr,] ## the test set
```

### Bootstrap
We can now proceed to the bootstraping with 100 replicates

```{r}
set.seed(897)
index.boot <- createResample(y=Mountain_df.tr_notsubs$Mountain_range, times=100)
head(index.boot[[1]])
tail(index.boot[[1]])
```

```{r}
df.boot.tr <- Mountain_df.tr_notsubs[index.boot[[1]],]
dim(df.boot.tr)

df.boot.val <- Mountain_df.tr_notsubs[-index.boot[[1]],]
dim(df.boot.val)
```

```{r}
# The count of the bootstraped df
ggplot(df.boot.tr) +
  geom_bar(aes(x = Mountain_range))
```

We can now replicate the models. 

### Neural Network Model (with Unique bootstraped data)
Simple hyperparameter tuning, this code takes time to run.

```{r cache = TRUE, message = FALSE, warning = FALSE, results = 'hide'}
set.seed(1)

nnetFit_boot <- train(Mountain_range ~ ., 
                 data = df.boot.tr,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

```

```{r}
plot(nnetFit_boot)
```

The best neural network should have 4 hidden units and a decay of 0.1. It is the same as before.

```{r}
set.seed(345)
nn4_boot <- nnet(Mountain_range ~ ., data=df.boot.tr, size=4, decay = 0.3)
```

```{r}
nn4_pred_boot <- predict(nn4_boot, df.boot.val, type="class")
```

```{r}
confusionMatrix(data=as.factor(nn4_pred_boot), 
                reference = df.boot.val$Mountain_range)
```

### KNN model (with Unique bootstraped data)

```{r}
set.seed(123)
KNN2 <- knn3(data=df.boot.tr,df.boot.tr$Mountain_range ~ ., k=2)
MR2.te.pred <- predict(KNN2, newdata = df.boot.val,type ="class") 

TAB2 <- table(Obs=df.boot.val$Mountain_range, Pred=MR2.te.pred) # confusion matrix 
TAB2
(ACC <- sum(diag(TAB2))/sum(TAB2)) # accuracy 
```
Now, the accuracy is of 95.89%, lower than before, but might be more realistic with the unique method and the bootstrap. 


--------------------------------


## Unsupervised learning method

### Cluster Analysis 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# New Dataframe: Select only the numerical values:
Mountain_data.num <- select_if(Mountain_data_cleaned, is.numeric)
to.delet <- c('Day', 'Month', 'Year')
Mountain_data.num <- select(Mountain_data.num, -to.delet)
```

```{r}
row.names(Mountain_data_cleaned) <- paste("M", c(1:nrow(Mountain_data_cleaned)), sep="") # row names are used after

# scaling the data 
Mountain_data_cleaned[,-c(1:9)] <- scale(Mountain_data_cleaned[,-c(1:9)])
```


#### Agglomerative Hierarchical Clustering (AGNES) with Manhattan distance

```{r message = FALSE}
set.seed(123)

# matrix of Manhattan distances 
mountain.d <- dist(Mountain_data_cleaned[,-c(1:9)], method = "manhattan") 

# create a data frame of the distances in long format
mountain.melt <- melt(as.matrix(mountain.d))

ggplot(data = mountain.melt, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() 

#dendrogram using a complete linkage.
mountain.hc <- hclust(mountain.d, method = "complete")
plot(mountain.hc, hang=-1)

#cut the tree to 4 clusters
plot(mountain.hc, hang=-1)
rect.hclust(mountain.hc, k=4)
mountain.clust <- cutree(mountain.hc, k=5)


#Interpretation of the clusters
mountain.comp <- data.frame(Mountain_data.num, 
                            Clust=factor(mountain.clust)
                            ,Id=row.names(Mountain_data_cleaned))

mountain.df <- melt(mountain.comp, id=c("Id", "Clust"))
head(mountain.df)

ggplot(mountain.df, aes(y=value, group=Clust, fill=Clust)) +
  geom_boxplot() +
  facet_wrap(~variable, scale="free")

#number of clusters
fviz_nbclust(Mountain_data_cleaned[,-c(1:9)],
             hcut, hc_method="complete",
             hc_metric="manhattan",
             method = "silhouette", 
             k.max = 25, verbose = FALSE)

mountain.pam <- pam(Mountain_data_cleaned[,-c(1:9)], k=5)
mountain.pam
plot(silhouette(mountain.pam))
```


