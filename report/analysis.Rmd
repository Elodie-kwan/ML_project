
```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

# Analysis part of the project 

## Import dataset
Let's import the cleaned dataset that we created. 

```{r}
Mountain_data_cleaned <- read.csv("../data/Mountain_data_cleaned.csv")

Mountain_data_cleaned$Country <- as.factor(Mountain_data_cleaned$Country)
Mountain_data_cleaned$Mountain_range <- as.factor(Mountain_data_cleaned$Mountain_range)
Mountain_data_cleaned$Locality <- as.factor(Mountain_data_cleaned$Locality)
Mountain_data_cleaned$Plot <- as.factor(Mountain_data_cleaned$Plot)
Mountain_data_cleaned$Subplot <- as.factor(Mountain_data_cleaned$Subplot)

Mountain_data_cleaned$Date <- as.Date(Mountain_data_cleaned$Date)
```

## Replace the NAs by the mean of the closest observations
```{r}
mean_for_fill <-colMeans( Mountain_data_cleaned %>%
  select(Plot,Glu_P)%>%
  filter(Plot == c("76", "77"))%>%na.omit()%>% select(Glu_P))
  
Mountain_data_cleaned[is.na(Mountain_data_cleaned)] <- mean_for_fill
rm(mean_for_fill)
```

## Splitting the data into Traning set and Test set 

```{r}
set.seed(123) ## for replication purpose

## the index of the rows that will be in the training set
index.tr <- sample(1:nrow(Mountain_data_cleaned), replace=FALSE,
                   size=0.75*nrow(Mountain_data_cleaned))

Mountain_data.tr <- Mountain_data_cleaned[index.tr,] ## the training set
Mountain_data.te <- Mountain_data_cleaned[-index.tr,] ## the test set
```

## Neural Network Model 

```{r}
# two nodes in the middle layer 
nn1 <- nnet(Mountain_range ~ ., data=Mountain_data.tr, size=2)

pred1 <- predict(nn1, type="class")
tab1 <- table(Obs=Mountain_data.tr$Mountain_range, Pred=pred1) # confusion matrix 
(acc1 <- sum(diag(tab1))/sum(tab1)) # accuracy 
```

Simple hyperparameter tuning, this code takes time to run.

```{r cache = TRUE, message = FALSE, warning = FALSE, results = 'hide'}
set.seed(1)
fitControl <- trainControl(method = "cv", 
                           number = 10)

nnetGrid <-  expand.grid(size = seq(from = 1, to = 6, by = 1),
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))

nnetFit <- train(Mountain_range ~ ., 
                 data = Mountain_data.tr,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

```

```{r}
plot(nnetFit)
```

The best Neural Networks parameters would be to choose 5 hidden layers, with a decay of 0.5. 

The manually written Neural Network model 
```{r}
nn5 <- nnet(Mountain_range ~ ., data=Mountain_data.tr, size=5, decay = 0.5)

pred5 <- predict(nn5, type="class")
tab5 <- table(Obs=Mountain_data.tr$Mountain_range, Pred=pred5) # confusion matrix
tab5
(acc5 <- sum(diag(tab5))/sum(tab5)) # accuracy

```

Here it says that it has perfect accuracy.

Some transformations in order to use the function **neuralnet** in R. 

```{r}
Mountain_data.class <- class.ind(Mountain_data_cleaned$Mountain_range)
colnames(Mountain_data.class) <- c("Central_Andes", "Central_Pyrenees", "Sierra_de_Guadarrama")
Mountain_data.class <- cbind(Mountain_data_cleaned[,10:34], Mountain_data.class)
head(Mountain_data.class)



Mountain_data.class.tr <- Mountain_data.class[index.tr,] ## the training set
Mountain_data.class.te <- Mountain_data.class[-index.tr,] ## the test set
```


```{r}
f <- as.formula(paste(
  "Central_Andes+Central_Pyrenees+Sierra_de_Guadarrama~", 
  paste(names(Mountain_data.class.tr[ ,1:25]),
              collapse = " + ")
  ))
f

```

```{r}
neuralnet5 <- neuralnet(f, data=Mountain_data.class.tr, hidden=c(5,3))

plot(neuralnet5, rep="best")
```


## Random Forest 
```{r}
train_set_for_RF <- Mountain_data.tr %>%
  select(!c(Plot, Subplot, Date, Day, Month, Year, Locality, Country))
test_set_for_RF <- Mountain_data.te%>%
  select(!c(Plot, Subplot, Date, Day, Month, Year, Locality, Country))

rf <- randomForest( Mountain_range ~ .,data=train_set_for_RF)

pred = predict(rf, newdata=test_set_for_RF[-1])
               
cm = table(test_set_for_RF[,1], pred)

varImpPlot(rf)
importance(rf)


```

```{r,  results='asis'}
cm

```


## Naive Bayes 

Conditional density Plots with 'usekernel=TRUE' 
```{r}
mountain.nb <- naive_bayes(Mountain_range ~ .,
                       data = Mountain_data.tr, usekernel=TRUE, laplace=1)
par(mfrow=c(2,2))
plot(mountain.nb, arg.num = list(col = 1:3,
                             legend.position = "topright",
                             legend.cex = 0.8),
     prob="conditional")
par(mfrow=c(1,1))
mountain.nb
```
Conditional density plots with 'uskernel=FALSE'. It means that Gaussian distribution is applied to 'numeric' vairable. 
```{r}
mountain.nb <- naive_bayes(Mountain_range ~ .,
                       data = Mountain_data.tr, usekernel=FALSE, laplace=1) 
par(mfrow=c(2,2))
plot(mountain.nb, arg.num = list(col = 1:3,
                             legend.position = "topright",
                             legend.cex = 0.8), prob="conditional")
par(mfrow=c(1,1))
mountain.nb
```

pH_T is medium in Central Andes, lower in Sierra de Guadarrama and higher in Central Pyrenees. 


Predictions 
```{r}
head(predict(mountain.nb, newdata = Mountain_data.tr[-c(1:9)]),20)
     
head(predict(mountain.nb, newdata = Mountain_data.tr[-c(1:9)], type="prob"),10)
```
Confusion matrix train data set 
```{r}
p1 <- predict(mountain.nb, Mountain_data.tr[-c(1:9)])
(tab1 <- table(p1, Mountain_data.tr$Mountain_range))
sum(diag(tab1)) / sum(tab1)
```
Confusion Matrix test data set 
```{r}
p2 <- predict(mountain.nb, Mountain_data.te[-c(1:9)])
(tab2 <- table(p2, Mountain_data.te$Mountain_range))
sum(diag(tab2)) / sum(tab2)
```

## K-NN Model

We use a 2-NN to predict the test set using the training set

```{R}
library(caret)

KNN <- knn3(data=Mountain_data.tr,Mountain_data.tr$Mountain_range ~ ., k=2)
MR.te.pred <- predict(KNN, newdata = Mountain_data.tr,type ="class") 

TAB <- table(Obs=Mountain_data.tr$Mountain_range, Pred=MR.te.pred) # confusion matrix 
TAB
(ACC <- sum(diag(TAB))/sum(TAB)) # accuracy 
```
